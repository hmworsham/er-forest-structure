{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning East River Stem Geolocation Shapefiles\n",
    "**Author:** 'Marshall Worsham' <br>\n",
    "**Creation Date:** '09/21/2020' <br>\n",
    "**Revision Date:** '12/22/2020' <br>\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1 - [Front matter](#front)<br>\n",
    "2 - [Import stem geolocation shapefiles](#import)<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Front matter<a id='front'></a>\n",
    "\n",
    "This notebook contains markdown and code for post-processing point shapefiles generated from Trimble Geo7X GPS acquisitions in the East River domain. The input is a set of shapefiles containing tree geolocation points, one set for each site in the watershed where stem geolocations were acquired from 2018â€“2020.\n",
    "\n",
    "The script appends the `Site` name and `subdirectory` to each shapefile name, then selects all projected point shapefiles, groups them by `Site` name, and merges points from the same site. It then filters out undesired points (e.g., plot corners and plot edges)\n",
    "\n",
    "The script was developed in `Python 3.8.2` on a Macbook Pro 2014 running OSX 10.14.6.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries<a id='libraries'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "from matplotlib import pyplot as plt\n",
    "from os.path import join, getsize\n",
    "%matplotlib inline\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the working directories and list contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.sep.join(['/Volumes', 'GoogleDrive', 'My Drive', 'Research', 'RMBL', 'Working_Files', 'Forest_Inventory_Dataset'])\n",
    "source_dir = os.sep.join([directory, 'Source'])\n",
    "scratch_dir = os.sep.join([directory, 'Scratch'])\n",
    "out_dir = os.sep.join([directory, 'Output'])\n",
    "stem_dir = os.sep.join([scratch_dir, 'GPS_Data_2021_MERGEDBYPLOT'])\n",
    "os.listdir(stem_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set pandas view to max rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import stem geolocation shapefiles<a id='import'></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the temp geodataframes from the append operations\n",
    "stem_files = os.listdir(stem_dir)\n",
    "stem_sf = [s for s in stem_files if s.endswith('.shp')]\n",
    "stem_paths = [stem_dir + s for s in stem_sf]\n",
    "appd_gpdf = []\n",
    "for sf in stem_sf:\n",
    "    gpdf = gpd.read_file(os.sep.join([stem_dir, sf]))\n",
    "    appd_gpdf.append(gpdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a figure frame\n",
    "plt.figure(figsize = (30,15))\n",
    "\n",
    "# Plot the dataframes in sequence on the figure frame\n",
    "for i, gpdf in enumerate(appd_gpdf):\n",
    "    \n",
    "    # create subplot axes in a 5x3 grid\n",
    "    ax = plt.subplot(5, 4, i + 1) # nrows, ncols, axes position\n",
    "    \n",
    "    # set title\n",
    "    ax.set_title(gpdf.Site[0])\n",
    "    \n",
    "    # plot the plot on these axes\n",
    "    gpdf.plot(ax=ax)\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some functions to interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print ordered list of new geodataframes\n",
    "print([i for i in stem_files if i.endswith('.shp')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # function to interact with dataframe\n",
    "# def fh_interact(df):\n",
    "#     '''\n",
    "#     outputs sliders that show rows and cols of df\n",
    "#     '''\n",
    "#     def peek(row = 1, col = 0):\n",
    "#         return df.iloc[row: row+10, col: col+10]\n",
    "#     interact(peek, row = (0, len(df), 5), col = (0, len(df.columns) - 6))\n",
    "#     print('({} rows, {} columns total'.format(df.shape[0], df.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function to generate new points based on geotag association notes in field notebooks and GPS units\n",
    "For some trees that were growing very closely together, we did not geotag each tree individually with the GPS unit in order to save battery and complete the inventory. Instead, we left directional notes in the `Other` or `Other2` field. These comments had the format, e.g. \"6030 0.4m NW\". This indicated that for a given geotagged tree, #6030 was an ungeotagged associated tree standing 0.4m to the northwest of the geotagged tree. We define a function below that uses those notes to construct geolocation points for the trees that were associated through these notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makenewpoint(associationdf, reftag, targtag, direction, distance):\n",
    "    '''\n",
    "    Creates new point geometries for stems that weren't geotagged but have directional and distance references in notes. \n",
    "    Inputs:\n",
    "        - gpdf: geodataframe storing geometry and association data\n",
    "        - reftag: an integer indicating the reference (geotagged) tree tag number\n",
    "        - targtag: an integer indicating the target (untagged associated) tree tag number\n",
    "        - dir: string indicating cardinal direction from reference to target (e,n,w,s,ne,nw,sw,se) noted in 'Other' or 'Other2' field\n",
    "        - distance: float indicating distance from reference to target noted in 'Other' or 'Other2' field\n",
    "    Returns:\n",
    "        - geopandas entry with point geometry\n",
    "    '''\n",
    "\n",
    "    # Conversion from degrees to radians\n",
    "    rad = np.pi/180\n",
    "\n",
    "    # Calculate cardinal directions as radian angles from east-facing origin\n",
    "    erad = rad*0\n",
    "    nrad = rad*90\n",
    "    wrad = rad*180\n",
    "    srad = rad*270\n",
    "    nerad = rad*45\n",
    "    nwrad = rad*135\n",
    "    swrad = rad*225\n",
    "    serad = rad*315\n",
    "\n",
    "    # Define radian direction based on cardinal direction input\n",
    "    if not isinstance(direction, float):\n",
    "        if direction.lower() == 'e':\n",
    "            raddir = erad\n",
    "        elif direction.lower() == 'n':\n",
    "            raddir = nrad\n",
    "        elif direction.lower() == 'w':\n",
    "            raddir = wrad\n",
    "        elif direction.lower() == 's':\n",
    "            raddir = srad\n",
    "        elif direction.lower() == 'ne':\n",
    "            raddir = nerad\n",
    "        elif direction.lower() == 'nw':\n",
    "            raddir = nwrad\n",
    "        elif direction.lower() == 'sw':\n",
    "            raddir = swrad\n",
    "        elif direction.lower() == 'se':\n",
    "            raddir = serad\n",
    "    # elif isinstance(direction, float):\n",
    "    else:\n",
    "        intrad = rad*direction\n",
    "        raddir = intrad\n",
    "\n",
    "    # Find x,y coordinates of reference tag number\n",
    "    refcoords = associationdf[associationdf['Tag_Number'] == reftag].geometry\n",
    "    x = refcoords.x\n",
    "    y = refcoords.y\n",
    "\n",
    "    # Calculate coordinates of target tag number\n",
    "    x_prime = distance * np.cos(raddir) + x\n",
    "    y_prime = distance * np.sin(raddir) + y\n",
    "    \n",
    "    # print(targtag, raddir, x, y, x_prime, y_prime, sep='\\n')\n",
    "\n",
    "    # Create a new temp gpdf from new geometries   \n",
    "    new_gdf = gpd.GeoDataFrame(geometry = gpd.points_from_xy(x_prime, y_prime, crs = 'epsg:32612'))\n",
    "    new_gdf['Tag_Number'] = targtag\n",
    "    new_gdf['Comment'] = 'Point generated post-campaign from geotag association note'\n",
    "    new_gdf['Geotag_Association'] = reftag\n",
    "    new_gdf = new_gdf[['Tag_Number', 'Comment', 'Geotag_Association', 'geometry']]\n",
    "\n",
    "    return new_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cleaning function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleansf(geodf, invsheet_id):\n",
    "    '''\n",
    "    TODO\n",
    "    '''\n",
    "    \n",
    "    # Prep inventory datasheet for merge\n",
    "    invsheet = f'https://docs.google.com/spreadsheets/d/{invsheet_id}/export?format=csv'\n",
    "    invdata = pd.read_csv(invsheet)\n",
    "    invdata['Tag_Number'] = pd.to_numeric(invdata['Tag_Number'], downcast='integer')\n",
    "    \n",
    "    # Prep points geodataframe for merge\n",
    "    gcrs = geodf.crs\n",
    "    geodf = pd.DataFrame(geodf)\n",
    "    geodf['Tag_Number'] = pd.to_numeric(geodf['Other'], downcast='integer')\n",
    "\n",
    "    # Merge the inventory and points dataframes\n",
    "    merged = invdata.merge(geodf, on='Tag_Number', how='left')\n",
    "    merged['Geotag_Association_Ref'] = pd.to_numeric(merged['Geotag_Association_Ref'], downcast='integer')\n",
    "    merged = gpd.GeoDataFrame(merged, crs = gcrs, geometry = merged['geometry'])\n",
    "\n",
    "    # return merged\n",
    "\n",
    "    # \n",
    "    def getinputs(df, col, testcol='Geotag_Association_Ref'):\n",
    "            return [df[col][i] for i in df.index if pd.notnull(df[testcol][i])]\n",
    "\n",
    "    reftags = pd.to_numeric(getinputs(merged, 'Geotag_Association_Ref'), downcast='integer')\n",
    "    targtags = pd.to_numeric(getinputs(merged, 'Tag_Number'), downcast='integer')\n",
    "    dirs = getinputs(merged, 'Geotag_Association_Dir')\n",
    "    dists = getinputs(merged, 'Geotag_Association_Dist')\n",
    "\n",
    "    assert len(reftags) == len(targtags) == len(dirs) == len(dists)\n",
    "    \n",
    "    newpoints_ls = []\n",
    "    #iterators = zip(reftags, targtags, dirs, dists)\n",
    "    \n",
    "    for targtag, reftag, dirc, dist in zip(targtags, reftags, dirs, dists):\n",
    "        if (pd.notnull(dirc) | pd.notnull(dist)):\n",
    "            newpoint = makenewpoint(merged, reftag, targtag, dirc, dist)\n",
    "            newpoints_ls.append(newpoint)\n",
    "    \n",
    "    geodfclean = geodf.append(newpoints_ls, ignore_index=True)\n",
    "    geodfclean['Site'] = invdata['Site_Name'][0]\n",
    "    geodfclean['Sp_Code'] = np.NaN\n",
    "    if 'Comment' not in geodfclean:\n",
    "        geodfclean['Comment'] = np.NaN\n",
    "    if 'Geotag_Association' not in geodfclean:\n",
    "        geodfclean['Geotag_Association'] = np.NaN\n",
    "    geodfclean = geodfclean[[\n",
    "        'Site',\n",
    "        'Tag_Number',\n",
    "        'Sp_Code',\n",
    "        'Latitude',\n",
    "        'Longitude',\n",
    "        'GNSS_Heigh',\n",
    "        'Horz_Prec',\n",
    "        'Vert_Prec',\n",
    "        'Std_Dev',\n",
    "        'GPS_Time',\n",
    "        'GPS_Date',\n",
    "        'Rcvr_Type',\n",
    "        'Corr_Type',\n",
    "        'Max_PDOP',\n",
    "        'Max_HDOP',\n",
    "        'Geotag_Association',\n",
    "        'Comment',\n",
    "        'geometry'\n",
    "        ]]\n",
    "\n",
    "    clsv = geodfclean.sort_values(['Tag_Number', 'Horz_Prec'])\n",
    "    dropidx = geodfclean[geodfclean.duplicated(subset='Tag_Number', keep='first')].index\n",
    "    geodfclean.drop(dropidx, inplace=True)\n",
    "    geodfclean.reset_index(drop=True, inplace = True)\n",
    "\n",
    "    geodfclean.replace(['', 'None'], np.nan, inplace = True)\n",
    "    geodfclean=gpd.GeoDataFrame(geodfclean, crs=gcrs, geometry=geodfclean.geometry)\n",
    "    geodfclean['Tag_Number'] = pd.to_numeric(geodfclean['Tag_Number'], downcast='integer')\n",
    "    geodfclean['Geotag_Association'] = pd.to_numeric(geodfclean['Geotag_Association'], downcast='integer')\n",
    "\n",
    "    return geodfclean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getshid(url):\n",
    "    return(url.split('/')[5].split('?')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cccvn1 = getshid('https://docs.google.com/spreadsheets/d/1beSW4hYnmxxC2X3yPkKJsqP3gg33RH6qbB9DvVVcnUE?authuser=worsham%40berkeley.edu&usp=drive_fs')\n",
    "cccvn2 = getshid('https://docs.google.com/spreadsheets/d/1KqNXUVWMN76Wu3emK11ZC0JWEGafcARoyVFIXHWx7EI?authuser=worsham%40berkeley.edu&usp=drive_fs')\n",
    "cccvs1 = getshid('https://docs.google.com/spreadsheets/d/1KpHlGMiAbLpT8QIccMEAgAQ2vkRZq_5d9i0jRm-S7OE?authuser=worsham%40berkeley.edu&usp=drive_fs')\n",
    "ccemn1 = getshid('https://docs.google.com/spreadsheets/d/1CyY3OfzBoFVHCu1QhchbKn7Y8g1uVMQJOQMFriynvSM?authuser=worsham%40berkeley.edu&usp=drive_fs')\n",
    "sgnes1 = getshid('https://docs.google.com/spreadsheets/d/1veFBxhR0wD4Qu07ZYyNNQlzKg5J9YTNL8E1VML8u_gw?authuser=worsham%40berkeley.edu&usp=drive_fs')\n",
    "sgnes3 = getshid('https://docs.google.com/spreadsheets/d/1v5oJcULvbW-IYfvfLVFuCzUIT5mTbwtfYNAorns1E5M?authuser=worsham%40berkeley.edu&usp=drive_fs')\n",
    "srpvg1 = getshid('https://docs.google.com/spreadsheets/d/1zc-1B8T91BEd0tc1SvgUGFot-Dj6M02p1ZLoXYCPw2o?authuser=worsham%40berkeley.edu&usp=drive_fs')\n",
    "xxpln1 = getshid('https://docs.google.com/spreadsheets/d/1VQ8bGxJACttKfQ0V4Hbg3HIIybRA5Pw8yBXUpD2Kv28?authuser=worsham%40berkeley.edu&usp=drive_fs')\n",
    "xxpln2 = getshid('https://docs.google.com/spreadsheets/d/1q-wH_h-WbNHHqALjGv3KzaTKHfLuYkkTHR9uzMSPtcU?authuser=worsham%40berkeley.edu&usp=drive_fs')\n",
    "sgnes2 = getshid('https://docs.google.com/spreadsheets/d/1s3MBQ5UMkMlyjV1YVecahvDHZ_Oyy0EyCYSveYkg_ig?authuser=worsham%40berkeley.edu&usp=drive_fs')\n",
    "ccuc1 = getshid('https://docs.google.com/spreadsheets/d/1cNTpoD0S0X1rnptilEfLWDyYxaEhrwBuIbD7XlPC3L4?authuser=worsham%40berkeley.edu&usp=drive_fs')\n",
    "erbme1 = getshid('https://docs.google.com/spreadsheets/d/1FDIBHXM0Zg_X_vkc_Ryc7v4P7L-7mtdbRI53ucaPgaw?authuser=worsham%40berkeley.edu&usp=drive_fs')\n",
    "sgswr1 = getshid('https://docs.google.com/spreadsheets/d/1EcrhpG2qMAwvTYdAH9J9Gqriuuiwe1PBAUAyLXnAgts?authuser=worsham%40berkeley.edu&usp=drive_fs')\n",
    "wgwgm1 = getshid('https://docs.google.com/spreadsheets/d/1mlF02kwTQLeCzf9jnSpFL48cxiQdEg7elwr4tsAOPXU?authuser=worsham%40berkeley.edu&usp=drive_fs')\n",
    "erapl1 = getshid('https://docs.google.com/spreadsheets/d/1CxcYtqh7jplLEr9fx7u2jOIAiTkbebrX8_M3330Abxk?authuser=worsham%40berkeley.edu&usp=drive_fs')\n",
    "xxcar3 = getshid('https://docs.google.com/spreadsheets/d/1Oxz-exmF11akp2IEkO3rG1ZyIggHDrSyznZFKEpa2HI?authuser=worsham%40berkeley.edu&usp=drive_fs')\n",
    "xxcar1 = getshid('https://docs.google.com/spreadsheets/d/1NJVvhj4gnI5e3hdVvwuSiC7NyIWXGwVUgZGc3Gv9Fko?authuser=worsham%40berkeley.edu&usp=drive_fs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All observations in 2021\n",
    "#invsheets = [xxpln1, sgnes2, cccvs1, xxpln2, srpvg1, sgnes3, ergt1, ccuc1, xxcar3, sgnes1, cccvn2, xxcar1, cccvn1, ccemn1, erbme1, sgswr1, wgwgm1, erapl1]\n",
    "#idxs = list(range(0, len(invsheets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All observations from new sites in 2021\n",
    "invsheets = [cccvs1, xxpln2, srpvg1, sgnes3, xxcar3, sgnes1, cccvn2, xxcar1, cccvn1, ccemn1]\n",
    "idxs = [2, 3, 4, 5, 8, 9, 10, 11, 12, 13]\n",
    "geodfs = [appd_gpdf[i] for i in idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observations missed in 2020 and taken in 2021\n",
    "# invsheets = [xxpln1, sgnes2, ccuc1, sgswr1, wgwgm1]\n",
    "# idxs = [0,1,7,15,16]\n",
    "# geodfs = [appd_gpdf[i] for i in idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appd_gpdf[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_list = []\n",
    "for i in range(len(invsheets)):\n",
    "    print(idxs[i])\n",
    "    result=cleansf(geodfs[i], invsheets[i])\n",
    "    clean_list.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_list[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_list[4].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(geodfs)):\n",
    "    print('original:', geodfs[i].shape)\n",
    "    print('post:', clean_list[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Assemble cleaned dataframes into list and export shapefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CRS match\n",
    "[clean_list[i].crs == clean_list[i+1].crs for i in np.arange(len(clean_list)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in clean_list:\n",
    "    outpath = os.path.join(out_dir, 'Kueppers_EastRiver_Stem_Geolocations_WGS84UTM13N', i['Site'][0]+'_Stem_Geolocations_WGS84UTM13N.shp')\n",
    "    i.to_file(outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in clean_list:\n",
    "    outpath = os.path.join(source_dir, 'TreeCoords', i['Site'][0]+'_Stem_Geolocations_WGS84UTM13N.csv')\n",
    "    i.to_csv(outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in os.listdir(os.path.join(out_dir, 'Kueppers_EastRiver_Stem_Geolocations_WGS84UTM13N')):\n",
    "    if '.shp' in i:\n",
    "        gdf = gpd.read_file(os.path.join(out_dir, 'Kueppers_EastRiver_Stem_Geolocations_WGS84UTM13N', i))\n",
    "        outpath = os.path.join(source_dir, 'TreeCoords', i[:-4]+'.csv')\n",
    "        gdf.to_csv(outpath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eastriver",
   "language": "python",
   "name": "eastriver"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
