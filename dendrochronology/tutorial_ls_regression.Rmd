---
title: "Ordinary least-squares regressions: the basics"
author: "Marshall Worsham"
date: "12/06/2022"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr, quiet=T)
library(stats, quiet=T)
library(ggplot2, quiet=T)
```

## What this tutorial covers

1. What ordinary least-squares (OLS) regressions do
2. How they work conceptually
3. How they work mathematically
4. Building a univariate OLS from scratch
5. Model interpretation
6. Challenge: practice with tree-ring and streamflow data

### 1. What ordinary least-squares (OLS) regressions do
At the very highest level, a regression is a kind of model, which means that it is a representation of something in the world. A bit more specifically, a regression describes the relationship(s) between a dependent variable and one or more independent variables, and it does so in a way that is simpler than the data themselves.

This tutorial focuses on ordinary least-squares (OLS) regression, which is the most familiar and straightforward form, but there are others: polynomial, logistic, partial least-squares, ridge and lasso, and generalized additive regressions, to name a few. 

*Aside: You'll often hear the terms "least-squares regression" or "linear regression" or "linear model" used to describe OLS regression; in most cases you can assume they're interchangeable. You'll also hear the terms "response", "outcome", or "y" used for dependent variables and "explanatory", "predictor", or "x" used for independent variables. Assume equivalence.*

The output of an OLS regression is an equation that describes a line in space. This "regression line" is the line that makes the vertical distance from the data points to the line itself as small as possible.  In more technical language, it is the line that minimizes the *variance* between the data and the line itself. We'll unpack what *variance* means in more detail shortly. Sometimes you'll hear this line called the "line of best fit," which makes sense in so far as the line fits all the points as closely as possible.

This line, or equivalently, its equation, is your model. The line-equation says that there is a relationship between the response ($y$) and the explanatory ($x$) variables. Specifically, it says that this relationship is linear, which means that for every one-unit increase in $x$, $y$ can be expected to increase by a precisely determined amount. 

**tl/dr:** A regression is a simplified representation of some set of data. It yields an equation that describes a line, and this line is the one that minimizes the distance between all of the data points and itself. This line enables us to estimate the response, $y$, for a given value of $x$.

### 2. How OLS regressions work conceptually

Let's start with a simple example, using just one response variable and one explanatory variable. We'll use the Air Quality dataset, which comes built into R. This contains daily air quality measurements made in New York from May to Sep 1973, along with simultaneous measurements of solar radiation, wind speed, and temperature. We'll take air quality (ozone concentration [O3] in parts per billion) as the response and see if we can find a linear relationship with wind speed using OLS.

Go ahead and load up the data and look at the first few lines.
```{r}
# Load the dataset
aq <- airquality

# View the first few lines of data
head(aq)

# Take out NA values
aq <- aq[rowSums(is.na(aq))==0,]
```

Run the block below to create a scatterplot of O3 concentration v. wind speed.
```{r}
# Plot the explanatory and response variables as a scatterplot
g <- ggplot(aq, aes(x=Wind, y=Ozone)) + 
  geom_point(fill='red', color='white', pch=21)

g
```

It looks like there could be a relationship here, probably a negative one. You can imagine a lot of different lines that *could* potentially describe this relationship. Run the chunk below to visualize a few of them.

```{r}
# Generate a bunch of potential regression lines
line_slopes <- seq(0,-9)
line_intercepts <- seq(20, 200, 20)
line_cols <- rainbow(10)
si <- data.frame(line_slopes=line_slopes, line_intercepts=line_intercepts)

# Plot the lines on top of the data
g +
  geom_abline(data=si, aes(slope=line_slopes, intercept=line_intercepts, col=line_cols))
```
 
In fact, there are infinite such lines. But only one of them will minimize the distance between itself and all of the data points. This is the line we want to find. A statistician named Iñaki Úcar made this kind of fun dynamic graphic below to help visualize finding the best-fit line from an infinite set of possible lines. 

<center><img src='https://github.com/Enchufa2/ls-springs/blob/main/reg.gif?raw=true'></center>

In this graphic, the blue line is a possible regression line. The vertical gray lines represent the vertical distance between a given data point and the regression line. As the blue line oscillates, the gray lines get shorter or longer (depending on which side of the pivot the data points are on). The blue line stabilizes once the *sum of the lengths of all of those gray lines* is at a minimum. This is the least-squares regression line. 

Read back over this section and take a moment to internalize it. These are the conceptual keys to understanding OLS. 

### 3. How they work mathematically

As we demonstrated in Section 2, when we fit a regression to a set of points, we're assuming that there is some straight-line relationship between $y$ and $x$. The following equation describes that relationship: 

$ŷ_i = \beta_0 + \beta_1X_i + \epsilon_i$

$\beta_0$ and $\beta_1$ are the equation's coefficients. $\beta_0$ is the intercept—the place where the regression line crosses the y-axis, or the value of $ŷ$ when $x=0$. $\beta_1$ is the slope, or the amount by which $ŷ$ is estimated to increase when $x$ increases by 1. Sometimes you'll hear these called "model parameters"; they mean the same thing. Intercept and slope are always the parameters of an OLS model. $X_i$ is your explanatory variable; it's a vector (list) of values for $X$ from your data, where $i$ represents the $i$th observation. $\epsilon$ is something called random error, which we'll get to shortly. $ŷ$ (usually said "y-hat") is the estimated response, or the predicted value of your response variable.

Whatever you end up with for $\beta_0$ and $\beta_1$, the actual value of an observation in your dataset ($y_i$) is probably going to deviate from the value that your line predicts, ($\hat{y}_i$). The differences between the actual and predicted values are called random errors, or residuals. (Remember those gray lines in the animation above?)

<center><img src="https://www.jmp.com/en_us/statistics-knowledge-portal/what-is-regression/the-method-of-least-squares/_jcr_content/par/styledcontainer_2069/par/lightbox_4392/lightboxImage.img.png/1548351202284.png"></center>

Except in the extremely unlikely case that all of your data sit perfectly on a straight line, you're always going to have some random error. Like suffering, it's something to accept. But remember that to fit a regression, we need to minimize the distance between all the data points and the line. This is the same thing as saying that we need to minimize the random error (or minimize the residuals).

When we fit a line through data, some of the errors will be positive and some will be negative. In other words, some of the actual values will be larger than their predicted value (they will fall above the line), and some of the actual values will be less than their predicted values (they'll fall below the line).

If we added up all of these errors, the sum would be zero. How then can we measure overall error? We use a little trick: **we square the errors, sum them up, and find a line that minimizes this sum of squared errors.**

There's some heavy-duty linear algebra and differentiation that can do this work and get us to the exact values of the coefficients $\beta_0$ and $beta_1$ (or what's known as the "true model"). But, as it turns out, there are some shortcut equations that can get us to a very, very close approximation of the true model, and that's usually good enough. We'll call these approximate coefficients $\hat{\beta}_0$ and $\hat{\beta}_1$ ("beta-hat") because we're estimating their values.

The shortcut equation for finding $\hat{\beta}_0$ is:

$$\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}$$

$\bar{y}$ is the mean of the $y$ values in your dataset. $\bar{x}$ is the mean of the $x$ values in your dataset. 

To find $\hat{\beta}_0$:

$$\hat{\beta}_1 = \frac{\sum{^n_{i=1}(x_i-\bar{x})(y_i-\bar{y})}}{\sum{^n_{i=1}(x_i-\bar{x})^2}}$$

Here, $x_i$ and $y_i$ are individual observations in your dataset and $\bar{x}$ and $\bar{y}$ are the averages of $x$ and $y$, respectively. 

In plain English, all you do to find $\hat{\beta}_1$ is:  

1. subtract each observed $x$ from the mean of $x$ and multiply this difference by the difference between each observed $y$ and the mean of $y$; repeat for each value of $(x,y)$,
2. subtract each observed $x$ from the mean of $x$ and square it; repeat for each value of $x$
3. sum the results of (1) and divide them by the sum the results of (2).

### 4. Building a univariate OLS from scratch

It's pretty straightforward to translate these equations into code. Once we do that, we can build a regression function that will allow us to run an OLS on any dataset. The end result of this section will be a function much like `lm()`, which is the basic linear model function that everyone uses from the {stats} package in R. 

First, let's translate those $\beta$ equations into code. (This is important! Spend some time with this, and make sure you know what is being computed at each step. Hint: take each step out of the function and print the results using some toy data for $x$ and $y$).

```{r}
ols <- function(x,y){
  #####
  # function get_betas takes two arguments, x and y
  # x: a single vector of values on one explanatory variable
  # y:  a single vector of values on one response variable
  # returns: a list of length two containing estimated values for b0 and b1
  #####
  
  # find the mean of x, but ignore NAs, which otherwise mess up the calculation
  mean_x = mean(x, na.rm=T) 
  
  # find the mean of y, but ignore NAs, which otherwise mess up the calculation
  mean_y = mean(y, na.rm=T)
  
  # calculate b1
  # note that x and y here are the vectors we named as arguments to the functon
  b1 = sum((x-mean_x) * (y-mean_y), na.rm=T)/sum((x-mean_x)^2, na.rm=T)
  
  # calculate b0
  b0 = mean_y - b1 * mean_x
  
  # return a result
  return(list(b0=b0, b1=b1))
}

```

So now we have a way to estimate the coefficients. We can apply this function to our air quality dataset to estimate the least-squares regression line that describes the relationship between air quality and wind speed.

```{r}
# Define y
y = aq$Ozone

# Define x
x = aq$Wind

# Compute coefficients
betas <- ols(x, y)

# Print the coefficients
print(betas)
```

When you run this, your intercept ($\hat{\beta}_0$) should be about 99.0 and slope ($\hat{\beta}_1$) should be about -5.7. The equation, or model, that describes the linear relationship between air quality and wind speed is, therefore:

$$\hat{y} = 99.0 - 5.7X_i$$
To visualize how our line fits the data, we can add it to that scatterplot. How well does this match your expectations?

```{r}
# to add a regression line, use the call geom_abline(). this takes arguments for intercept in slope, for which we can just plug in the results from get_betas()
g + geom_abline(intercept=betas$b0, slope=betas$b1, col='blue3')
```

The line passes through the data about as you might expect, and there's a more or less equal balance between data points falling above the line and falling below the line, so at first glance it looks like we produced a valid model. Great.

### 5. Model interpretation

Now, how do we interpret what our model says? How do we go from coefficient values to saying something more interesting about the world? Well, first we can think about what the coefficients mean. With $\beta_0=99.0$, we can say that if the wind speed is zero, we should expect New York's baseline, steady-state O3 concentration (in 1973, anyway... ) to be about 99 parts per billion (ppb). That is, in a parcel of air containing a billion molecules, 99 of them will be O3. 

What of $\beta_1$? Our model suggests that for every 1 mph increase in wind speed, we should expect O3 concentration to decrease by about 5.7 ppb. 

**Challenge question: Why would O3 drop with higher wind speeds?**

But how good is our model, actually? Clearly there are some big differences between our observed data and what the regression line predicts, especially at lower wind speed values. How big are these differences? To answer that, we can compute the predicted O3 concentration ($\hat{y}_i$) at a couple of different wind speeds ($\x_i$. Here's a quick function to compute $\hat{y}_i$:

```{r}
# Quick function to predict y-hat at any given x
predict_y <- function(x, b0, b1){
  
  y_hat <- b0 + b1*x
  
  return(y_hat)
}
```

We can use that to predict y_hat at a couple of wind speeds
```{r} 
# Predict y-hat at a couple of values of x
x_i <- aq$Wind[c(22, 117)] # Wind speed at the 22nd and 117th observations
print(x_i)

y_hat_i <- predict_y(x_i, betas$b0, betas$b1)
print(y_hat_i) # Predicted O3 at the 22nd and 117th observations

y_obs_i <- aq$Ozone[c(22,117)]
print(y_obs_i) # Observed O3 at the 22nd and 117th observations
```

As the chunk above shows, when wind speed was around 16.6 mph, the actual observed O3 concentration was 11 ppb. Our model would have predicted about 13 ppb, so not too far off. But when wind speed was 3.4 mph, observed O3 was 168 ppb, while our model would have predicted a much lower value, around 70 ppb. Here's a visual:

```{r}
# Observed v. model-predicted O3 concentration when Wind=3.4 mph and 16.6 mph
print(paste('Observed:', y_obs_i))
print(paste('Predicted:', y_hat_i))

xy_df <- data.frame(x_i, y_hat_i, y_obs_i)

# Visualize the random error (residual) for this datapoint
g + 
  geom_abline(intercept=betas$b0, slope=betas$b1, col='blue3') +
  geom_segment(aes(x=x_i, xend=x_i, y=y_obs_i, yend=y_hat_i, color='error'), col='black', data=xy_df) + 
  ggtitle('OLS regression on Ozone and Wind Speed, with random error shown for two observations')
```

To get a handle on the strength, or explanatory power, of our model, we need to have some way of estimating how well the regression line fits *all* of the data, not just a couple of points. We'll develop another function to figure this out. In this case, we'll be computing a statistic called $R^2$, which you've probably come across before. This will give us a quantitative estimate of how close, on average, our predictions would be to the observed values in our data. In other words, it will tell us how much explanatory power our model has. 

$R^2$ relies on this pretty straightforward calculation:

$$R^2 = 1 - \frac{Unexplained \ variance}{Total \ variance} = 1 - \frac{\sum{^n_{i=1}}(y_i - \hat{y}_i)^2}{\sum{^n_{i=1}}(y_i - \bar{y})^2}$$

But implementing it requires a few steps. First, you use the regression coefficient to calculate predicted $\hat{y}_i$ values. Then you subtract the observed values, $y_i$ from $\hat{y}_i$, and square the results. The sum of these squared values is what's called the model's *unexplained variance*. Next, you subtract the mean of observed $y$ from each of the observed values $y_i$, and square the results. The sum of squares in the denominator is the *total variance*. Divide the unexplained variance by the total variance, subtract the result from one, and you have $R^2$.

It's worth saying explictly what these two types of *variance* mean. Total variance is the sum of squared differences between your observations and their mean, or the total amount that your observed response values vary around the average response. Unexplained variance is the portion of the total variance that your model fails to account for, or the sum of squared differences between your observations and the values that your model predicts. It should always be the case that unexplained variance is less than (or if your model really sucks, equal to) total variance.

**Challenge question: based on what you know about random errors (residuals), how do they relate to variance and $R^2$?**

Next, we'll implement the $R^2$ computation.
```{r}

# First we create the function for computing R^2
r2 <- function(y, y_hat){
  
  # Find the unexplained variance
  unex_v = sum((y-y_hat)^2, na.rm=T)
  
  # Find the total variance
  tot_v = sum((y-mean(y, na.rm=T))^2, na.rm=T)
  
  # Subtract the ratio of unexplained variance to total variance from 1
  rsq = 1-unex_v/tot_v
  
  # Return R^2 as the result
  return(rsq)
  
}
```

So that's our $R^2$ function. Next, we'll need to find the predicted values of y. For that, we'll use the `predict_y()` function that we defined above, which takes the original observed $X_i$ (in this case, wind speed) and uses our regression coefficients to estimate $\hat{y}_i$ at each $x$. Here we go.

```{r}

# Grab the vector containing observed x values from our dataset
X_i <- aq$Wind

# Predict y_hat
y_hat <- predict_y(X_i, betas$b0, betas$b1)

# Grab the vector of observed y values from our dataset
y_obs <- aq$Ozone

# Use the function to calculate R^2
model.r2 <- r2(y=y_obs, y_hat=y_hat)

print(model.r2)
```

So, our $R^2$ is only about 0.38, which means that our regression explains only 38% of the total variance in the data. The remaining 62%—which, again, represents the sum of the squares of random errors—is either the result of truly random variability in the world or can be accounted for by some variable we didn't include in the model.

In the next tutorial, on multivariate OLS, we'll see how adding additional explanatory variables can help to improve $R^2$.

The last thing we should do is see how our DIY regression model compares with the built-in R ordinary least-squares regression. The function that most researchers use in R is `lm()` from the package `stats`. ("lm" for linear model). We'll run our $X_i$ and $y_i$ through that function to see how the results compare with what we just did above.  

```{R}
# Run function lm() on y_observed and X_i
rolm <- lm(y_obs ~ X_i)
print(summary(rolm)$coef)
print(summary(rolm)$r.squared)
```

And it's bang on. `lm()` labels the coefficients "Estimates" because, remember, we're just estimating the coefficients with those equations. It also reports the standard error of the estimates, which we haven't calculated here. Your coefficients and $R^2$ are the key things you'll report any time you run a linear regression. But there are other results that are worth looking at, too. Let's look at the full summary of `lm()` results.

```{R}
# Look at other summary statistics
print(summary(rolm))
```

As you can see, there are a few other statistics in the `lm()` reports. The *Residuals* section presents some summary statistics describing the distribution of residuals, i.e. the difference between observed response values and model-predicted response values, or $y_i - \hat{y}_i$. These summary stats include the minimum, median, and maximum differences, as well as the difference at the first quartile (or 25th percentile) and third quartile (75th). 
We can find the same values on our own. 

```{r}
# Calculate residuals for each observation, y_i
resid <- y_obs - y_hat

# Print a summary of residuals
print(summary(resid))

# Compare to summary of lm() results
print(summary(rolm$residuals))
```

Another important set of statistics in the `lm()` summary is *t value* and *Pr(>|t|)*. These describe the outcome of a significance test, which asks whether the coefficient value (*Estimate* in `lm()` parlance) is significantly different from 0. That is, is there actually a linear trend between the response and explanatory variable under consideration, within a certain degree of confidence?

In a subsequent tutorial I'll walk through how to perform and interpret that significance test in detail. But for now, it's enough to mention what $t$ and $PR(>|t|)$ are. $t$ is the coefficient estimate divided by its standard error ($\frac{\beta}{SE_\beta}$). $PR(>|t|)$ is the $p$ value corresponding to the computed $t$, or the probability that $t$ resulted by chance alone. If $p$ is less than some specified significance level (a value that you pre-define before running the test, usually 0.1, 0.05, or 0.01), then you can conclude that the coefficient is significant within that significance level. In our example case, the $p$ value for wind speed is much smaller than 0.01 (infinitesimally small, in fact), so we can conclude that the inferred relationship between air quality and wind speed is significant. 

### 6. Challenge
Your challenge task is to run a linear regression using the built-from-scratch functions (defined above) on some dummy tree-ring data (provided below) and discharge values from your streamflow data at one gauging station. You should:

1. Plot your explanatory and response variables.
2. Report your coefficients ($\beta_0$ and $\beta_1$).
3. Add the regression line to the plot.
4. Report your $R^2$.

```{r}

# Ingest your streamflow time series as a dataframe and summarize monthly values to annual values.
# Hint: `read.csv()` from base R or `read_excel()` from the `readxl` package may be helpful.

# Some dummy tree ring width index (rwi) data. 
set.seed(999)
t.seq <- 1900:2019
rwi.chron <- data.frame(
  'Year'=t.seq,
  'rwi'= abs(0.2*sin(t.seq)+runif(t.seq))*100)

# Define your y_obs variable 
# Hint: grab the discharge values you want to use from your streamflow dataframe

# Define your x_obs variable (rwi values)
# Hint: the y_obs and x_obs should be exactly the same length and must match up year-by-year

# Create a scatterplot of y_obs against x_obs

# Run the regression on x_obs and y_obs using the `ols()` function we built above

# Plot the regression line on top of the scatterplot

# Predict y-hat using your model coefficients

# Calculate R^2

# Interpret the results in clear prose. At minimum, answer these questions:
## What is y if x=0? How much does y increase given a unit increase of x?
## How much variance does your model explain? 
## What might account for any unexplained variance?

````